"""
Critic Agent: Evaluates the summary and identifies missing information or areas for improvement
"""
import os
from openai import OpenAI
from utils.logger import agent_logger


class CriticAgent:
    def __init__(self):
        self.name = "Critic Agent"
        self.client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        agent_logger.info(f"{self.name} initialized")

    def critique(self, query: str, summary: str, chunks: list[str]):
        """
        Evaluate the summary and identify gaps or missing information

        Args:
            query: Original user question
            summary: Summary generated by SummarizerAgent
            chunks: Original retrieved chunks for reference

        Returns:
            dict with critique feedback
        """
        agent_logger.info(f"{self.name}: Starting critique for query='{query}'")
        agent_logger.debug(f"{self.name}: Summary length: {len(summary)} chars, {len(chunks)} chunks available")

        if not summary:
            agent_logger.warning(f"{self.name}: No summary provided for critique")
            return {
                "status": "error",
                "message": "No summary to critique",
                "critique": "",
                "suggestions": []
            }

        context = "\n\n---\n\n".join(chunks[:3])  # Use top 3 chunks for context

        prompt = f"""You are a quality checker. Evaluate if the answer is factually accurate and based ONLY on the provided context.

Original Question: {query}

Provided Answer:
{summary}

Available Context (from document):
{context}

CRITICAL CHECK:
1. Is the answer factually correct based on the context?
2. Did the answer add information NOT found in the context? (This is BAD - hallucination)
3. Did the answer miss important information that IS in the context?

FOR SIMPLE QUESTIONS:
- Check if answer directly matches what's in the context
- If answer added extra details not in context, mark as GAP: "Answer includes information not in document"
- If answer is accurate and concise, say "STRENGTHS: Accurate and from document. GAPS: None. SUGGESTIONS: None."

FOR COMPLEX QUESTIONS:
- Check if all information comes from the context
- Identify any hallucinated details (not in context)
- Identify missing details that ARE in the context

Your evaluation:
STRENGTHS: [What's correct from context]
GAPS: [Hallucinations OR missing context info]
SUGGESTIONS: [Stick to context OR add missing context info]"""

        try:
            model = os.getenv("LLM_MODEL", "gpt-4")
            agent_logger.info(f"{self.name}: ðŸ¤– Invoking LLM - Model: {model}, Temperature: 0.4")

            response = self.client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": "You are a critical evaluator focused on ensuring comprehensive and accurate answers."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.4
            )

            critique = response.choices[0].message.content

            # Log token usage
            usage = response.usage
            agent_logger.info(
                f"{self.name}: âœ… LLM Response received | "
                f"Tokens: {usage.prompt_tokens} input + {usage.completion_tokens} output = {usage.total_tokens} total"
            )

            # Parse suggestions (basic extraction)
            suggestions = []
            if "SUGGESTIONS:" in critique:
                suggestions_text = critique.split("SUGGESTIONS:")[-1].strip()
                suggestions = [s.strip() for s in suggestions_text.split("\n") if s.strip() and not s.startswith("STRENGTHS") and not s.startswith("GAPS")]

            has_gaps = "GAPS:" in critique and len(critique.split("GAPS:")[-1].strip()) > 10
            agent_logger.info(f"{self.name}: Critique analysis - has_gaps={has_gaps}, {len(suggestions)} suggestions")

            return {
                "status": "success",
                "critique": critique,
                "suggestions": suggestions[:5],  # Top 5 suggestions
                "has_gaps": has_gaps
            }

        except Exception as e:
            agent_logger.error(f"{self.name}: Error during critique: {str(e)}", exc_info=True)
            return {
                "status": "error",
                "message": f"Error during critique: {str(e)}",
                "critique": "",
                "suggestions": []
            }
